# Project 2 - Ames Housing Data and Kaggle Challenge

This research consisted two DataFrames. One dataframe had 81 columns including saleprice, which is our target variable. However, our other data does not have a saleprice column. Using the first data, I was assigned to make a train test split and run various amounts of modeling to come up with my best predictions for the second dataframe. 

First, I used a simple Liner Regression to see if I could get a baseline for what the saleprice would be close to. Although overall result came out to be that about 67.59% of the variance is explained by the x-variables in our model. It was a good start to base our research on. 

Using Kaggle and cross-val-scores, we are able to decipher whether we are getting close to the desired saleprice. At first, I was not getting nearly good scores. I consistantly ran under top 10 in the leaderboard. Wondering why it was sucha low score. But, I realized that I was underfitting the data. Although there are more variables I could be inputting to see what the score could be, I only included the ones that were easily manipulatable. For example, I used 8 variables to start off: ['lot_area', '1st_flr_sf', '2nd_flr_sf', 'full_bath', 'half_bath', 'pool_area', 'total_bsmt_sf', 'garage_area']. These were my initial 8. But, looking back at the data, I was able to see that there were three more: ['garage_cars', 'overall_qual', 'overall_cond']. Including this, it brought my score up from 13th place to 3rd. Also, instead of using K-Nearest Neighbors as the model to base my result off from, I used Lasso Regressor instead. 

As a comparison to the first Linear Regression, like I have stated before, it gave me 67.59%. After including the new sets of variables, I increased it to 77.21% (around 10% jump)! 
Using Ridge_cv, I received 87.07%, which is great! But, manipulating with it again and looking for Lasso Regression now, I got 87.56%. Although overall, I was not able to see whether Ridge would give me a better score due to limits of how many I could turn in per day, I believe looking at the cross validation score, I can safely assume that Lasso had the best result for the saleprice predictions.

Though my result isnt accurate, I am thrilled to say that the result I gave, ~87.56% of the variance can be explaine by the x-variables in the model. 

In conclusion, in order to predict the price of the housings, I recommend using the Lasso Regressor to predict. Also, I found that instead of using K Nearest Neighbors model, the lasso or ridge prediction came out to be the better predictor models. I would like to also note that instead of having just a few features, more variables are better for the model. Therefore, to predict the exact price of the housing, there could be a bit of improvement in the collected datasets and the usage of the model itself. One very simple method that could be used for the model to better predict is to overfit the data by bringing in all the features and drop the unnecessary ones that is causing the model to overfit.